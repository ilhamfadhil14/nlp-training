{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.error import HTTPError, URLError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running `BeautifulSoup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = urlopen('http://www.pythonscraping.com/pages/page1.html')\n",
    "bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "print(bs.h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bs.html.div)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Connection and Exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    html = urlopen('https://pythonscrapingthisurldoesnotexist.com')\n",
    "except HTTPError as e:\n",
    "    print(e)\n",
    "except URLError as e:\n",
    "    print('The server could not be found!')\n",
    "else:\n",
    "    print('It worked!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTitle(url):\n",
    "    try:\n",
    "        html = urlopen(url)\n",
    "    except HTTPError as e:\n",
    "        return None\n",
    "    try:\n",
    "        bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "        title = bs.body.h1\n",
    "    except AttributeError as e:\n",
    "        return None\n",
    "    return title\n",
    "\n",
    "\n",
    "title = getTitle('http://www.pythonscraping.com/pages/page1.html')\n",
    "if title == None:\n",
    "    print('Title could not be found')\n",
    "else:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Develop parsing function that can extract title and paragraph as a list object from [http://www.pythonscraping.com/pages/page1.html](http://www.pythonscraping.com/pages/page1.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getContent(url):\n",
    "\n",
    "    # type your code here\n",
    "\n",
    "    return content\n",
    "\n",
    "content = getContent('http://www.pythonscraping.com/pages/page1.html')\n",
    "\n",
    "if content == None:\n",
    "     print('Content could not be found')\n",
    "else:\n",
    "    for i in content:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def getContent(url):\n",
    "\n",
    "    # type your code here\n",
    "    try:\n",
    "        html = urlopen(url)\n",
    "    except HTTPError as e:\n",
    "        return None\n",
    "    try:\n",
    "        bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "        title = bs.body.h1\n",
    "        body = bs.body.div\n",
    "        content = []\n",
    "\n",
    "        content += title\n",
    "        content += body\n",
    "    \n",
    "    except AttributeError as e:\n",
    "        return None\n",
    "    return content\n",
    "\n",
    "content = getContent('http://www.pythonscraping.com/pages/page1.html')\n",
    "\n",
    "if content == None:\n",
    "     print('Content could not be found')\n",
    "else:\n",
    "    for i in content:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Parser\n",
    "\n",
    "Create parser that scrapes page located at [https://www.pythonscraping.com/pages/warandpeace.html](https://www.pythonscraping.com/pages/warandpeace.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = urlopen('https://www.pythonscraping.com/pages/warandpeace.html')\n",
    "bs  = BeautifulSoup(html.read(), 'html.parser')\n",
    "\n",
    "namelist = bs.findAll('span', { 'class' : 'green'})\n",
    "for name in namelist:\n",
    "    print(name.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Children and Other Descendants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = urlopen('https://www.pythonscraping.com/pages/page3.html')\n",
    "bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "\n",
    "for child in bs.find('table', {'id':'giftList'}).children:\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Siblings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = urlopen('https://www.pythonscraping.com/pages/page3.html')\n",
    "bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "\n",
    "for sibling in bs.find('table', {'id':'giftList'}).tr.next_siblings:\n",
    "    print(sibling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = urlopen('https://www.pythonscraping.com/pages/page3.html')\n",
    "bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "\n",
    "print(bs.find('img', \n",
    "              {'src' : '../img/gifts/img1.jpg'})\n",
    "              .parent.previous_sibling.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = urlopen('https://www.pythonscraping.com/pages/page3.html')\n",
    "bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "\n",
    "for item in bs.find('table', {'id' : 'giftList'}).findAll('tr', {'class' : 'gift'}):\n",
    "    print(item.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Extract `Item Title` information from [https://www.pythonscraping.com/pages/page3.html](https://www.pythonscraping.com/pages/page3.html) page and store into list object. You can expand it from previous method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change or update this script\n",
    "for item in bs.find('table', {'id' : 'giftList'}).findAll('tr', {'class' : 'gift'}):\n",
    "    print(item.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "for item in bs.find('table', {'id' : 'giftList'}).findAll('tr', {'class' : 'gift'}):\n",
    "    for gift in item.findAll('td')[0]:\n",
    "        print(gift.get_text())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8164b79054baafd32ac16b5e9f459bd79fbb7f855c6bedf6f48d0e0c944eb799"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
